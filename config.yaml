dataset: 'dataset/' # dataset directory
models: 'models/' # for storing models
plots: 'plots/' # for plots
pickle: 'pickle/' # pickle dumps root path

cuda: True #whether to use NVIDIA cuda
    
test_per_epoch: 0 #test per epoch i.e. how many times in ONE epoch
test_every_epoch: 50 #after how many epochs
print_per_epoch: 3 #print loss function how often in each epoch
save_every: 400 #save models after how many epochs
plot_every: 25 #save plots for test loss/train loss/accuracy

resume: True #resume training from saved model

lr: 0.0005 #learning rate

drop_uniform: False #whether dropping of character sets is independent of set size
reset_after: 400 #generate a new random dataset after these manh epochs
vocab_size: 26 #size of vocabulary. 26 engliush letters in our case
min_len: 3 #words with length less than min_len are not added to the dataset

rnn: 'GRU' #type of RNN. Can be LSTM/GRU
use_embedding: True #whether to use character embeddings
embedding_dim: 128 #if use_embedding, dimension of embedding
hidden_dim: 512 #hidden dimension of RNN
output_mid_features: 256 #number of neurons in hidden layer after RNN
miss_linear_dim: 256 #miss chars are projected to this dimension using a simple linear layer
num_layers: 2 #number of layers in RNN
dropout: 0.3 #dropout
batch_size: 4000 #batch size for training and testing
epochs: 3000 #total no. of epochs to train

training:
  max_episodes: 5000
  warmup_episode: 100
  save_freq: 100
  learning_rate: 0.001

optimizer:
  lr_min: 0.0001
  lr_decay: 1000

rl:
  epsilon_start: 1.0
  epsilon_end: 0.1
  epsilon_decay: 1000
  gamma: 0.99
  memory_size: 10000
  batch_size: 64
  target_model_update_episodes: 10
  max_steps_per_episode: 20
